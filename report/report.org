#+TITLE:     Rapport sur le projet de détection de liens entre messages par chaînes lexicales
#+AUTHOR:    Hugo Mougard
#+EMAIL:     hugo.mougard@etu.univ-nantes.fr
#+DATE:      dimanche 19 janvier 2014
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  fr
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:

* Design proposé
** Notes préliminaires
   La tâche a été découpée en trois sous-tâches distinctes :

   - construire un réseau de collocations ;
   - construire les chaînes lexicales correspondant aux messages à
     lier ;
   - lier les messages.

   Chaque sous-tâche fait l'objet d'un workflow à part entière. Ils
   sont détaillés tour à tour ci-dessous.
** Construction du réseau de collocations
   Le workflow proposé consiste en l'utilisation d'un Collection
   Reader et deux deux AEs pour remplir une ressource représentant le
   réseau de collocations visé. Il est fidèle au diagramme fourni :

   #+ATTR_HTML: width="800px"
   [[./img/cn.png]]

   Contrairement à l'approche théorique proposée plus loin, le CR n'a
   pas été modifié : il est toujours en charge de parser le HTML dans
   l'implémention proposée.
** Construction des chaînes lexicales
   Un des problèmes majeurs pour détecter les liens entre différents
   messages est de garder disponible l'information de tous les
   messages d'un thread durant le processing de chacun de ses
   messages. Pour résoudre ce problème, une approche basée sur les
   ressources a été implémentée : les informations sur les messages
   sont conservées dans une ressource partagée qui permet durant le
   =process= d'un message en particulier d'avoir à disposition toutes
   les informations nécessaires (ici les chaînes lexicales de ce
   message et de tous les messages du thread dont il fait partie).

   De plus, on peut avoir besoin d'informations qui ne seront
   disponibles qu'à un point ultérieur dans le traitement du corpus si
   l'on se base sur une architecture en pipeline de création des
   chaînes puis de liaison des messages. On voudrait avoir la
   possibilité de relier un message à tous les messages d'un thread,
   pas seulement ceux vus jusqu'au traitement du message à relier. Or
   au stade courant du traitement d'un message tous les messages de
   son threads qui ne sont pas encore apparus n'ont pas de chaînes
   lexicales produites et ne peuvent pas être reliés au message
   courant.

   On peut souhaiter cette capacité de relier un message à des
   messages ultérieurs par exemple pour voir si la méthode proposée
   est robuste aux mbox non ordonnées (ce qui n'est pas le cas pour le
   corpus considéré: la mbox est ordonnée).

   Ainsi, pour proposer une approche générale satisfaisant les
   contraintes mentionnées ci-dessus, la création de la ressource des
   chaînes lexicales fait l'objet d'un workflow à part entière. Cette
   décision de design entraîne deux répétitions : la première est la
   lecture du corpus de messages qui sera effectuée deux fois au lieu
   d'une, la seconde est le parsing des messages qui là encore sera
   effectué deux fois au lieu d'une. Ces inconvénients sont toutefois
   mineurs : ces deux étapes ne sont pas celles qui prennent le plus
   de temps dans cette chaîne de traitements.

   Le diagramme présenté ci-dessous détaille l'approche proposée pour
   créer la ressource de chaînes lexicales (il se lit de haut en bas
   et non de gauche à droite comme le diagramme proposé pour le réseau
   de collocations) :

   #+ATTR_HTML: width="800px"
   [[./img/cl.png]]

   Comme on peut le voir, le procédé est similaire à celui utilisé
   pour créer le réseau de collocations, avec toutefois une ressource
   supplémentaire (ce même réseau de collocations).
** Détection de lien entre les messages d'un même thread
   Une fois la ressource des chaînes lexicales disponibles,
   implémenter la détection de lien ne nécessite pas un procédé très
   compliqué. Le fichier digest des threads est utilisé (une fois mis
   sous forme de ressource) pour savoir quels messages l'on peut
   relier et une fois la détection du message optimal à relier
   effectuée, il est directement possible de remplier un fichier
   résultat. Cette approche est résumée ci-dessous par un dernier
   diagramme :

   #+ATTR_HTML: width="800px"
   [[./img/ld.png]]

   Une autre solution que celle retenue (la ressource partagée) pour
   avoir accès aux informations de tous les messages d'un thread
   pendant le =process= de chacun d'entre eux aurait été de créer un
   type Thread et de merger les CAS des messages en un CAS de
   Thread. Cette option n'a pas été retenue car les CAS Multiplier de
   UIMA ne facilitent pas ce genre d'opérations à l'heure actuelle et
   la méthode par ressource était plus facile à mettre en place. Il
   est cependant possible que merger les CAS soit plus efficace, en
   particulier en espace mémoire requis, si l'implémentation arrive à
   profiter pleinement des avantages théoriques de l'approche (ce qui
   est un objectif dur à atteindre avec les CAS Multipliers actuels).
* Design idéal
  TODO
* Design des ressources
** Ressource du réseau de collocations
   Cette ressource n'est pas détaillée car elle n'a pas évolué depuis
   le TP rendu début décembre.
** Ressoure des chaînes lexicales
   Cette ressource permet de garder un mapping des identifiants des
   messages vers leurs chaînes lexicales. C'est donc un simple wrapper
   autour d'une Map, à la manière de la ressource WordCounter vue en
   cours.
** Ressource d'info des threads
   Cette ressource expose les informations contenues dans le fichier
   thread digest par deux méthodes :

   - la première permet de récupérer l'identifiant d'un thread étant
     donné l'identifiant d'un message (l'identifiant d'un thread est
     l'identifiant de son premier message)

   - la seconde permet de récupérer les identifiants des messages d'un
     thread étant donné l'identifiant de ce thread.

   L'implémentation proposée utilise deux Map, l'une avec pour clefs
   les identifiants de messages, l'autre avec pour clefs les
   identifiants de threads.

   L'ensemble de ces méthodes offre une souplesse suffisante pour
   traiter le problème qui nous concerne.
* Design des composants
** AE de création du réseau de collocations
   Cet AE a été réimplémenté depuis le TP rendu début décembre. Il est
   maintenant conçu pour que la taille de la fenêtre soit paramètrable
   et le parcours de cette fenêtre se fait avec queue pour une
   efficacité optimale.
** AE de segmentation
   Une modification a été apportée pour ne garder que les mots de deux
   lettres ou plus constitués seulement de lettres (=p{L}{2,}=), pour
   limiter le bruit.
** AE de création des chaînes lexicales
   Cet AE a été implémenté pour que les expérimentations sur les
   chaînes lexicales soient faciles par la suite. Il est donc possible
   de paramétrer, en plus des deux ressources nécessaires (le réseau
   de collocation et la ressource “output” des chaînes lexicales) :

   - le fossé maximal entre deux mots pour qu'ils puissent être
     considérés en relation de collocation ;

   - le score de collocation à partir duquel on considère que deux
     mots sont en relation de collocation ;

   - la longueur minimale des chaînes lexicales retenues.

   La création des chaînes lexicales se fait en ne visitant chaque
   mot du message qu'une fois et en l'intègrant ou non aux chaînes
   lexicales existantes. Les chaînes lexicales ne sont pas mergées.

   /i.e./, si on a les chaînes lexicales ={soleil, pluie}= et ={commerce}=
   au pas 2 et qu'on rencontre =vente= au pas 3, qui pourrait
   s'intégrer aux deux chaînes, on ne les regroupe pas. D'une part
   pour une question de performance et d'autre part parce que le merge
   n'est pas toujours justifié, comme on le voit avec cet exemple.

   Une fois ces chaînes créées, elles sont ajoutées à la ressource
   =LexicalChainModel= qui permet de retrouver les chaînes lexicales
   d'un message depuis son messageId.
** AE de détection de liens entre messages
   Cet AE utilise une ressource qui rend disponible le thread digest,
   en plus de la ressource contenant les chaînes lexicales. Pour
   déterminer le meilleur message à lier au message courant, on lui
   compare tous les autres messages de son thread et on lui lie le
   message d'avec lequel la comparaison a donné le meilleur score.

   L'algorithme utilisé pour la comparaison de deux ensembles de
   chaînes lexicales (et donc deux messages) est extrêmement basique
   (car il a été dit que l'intérêt du projet ne résidait pas dans
   l'approche théorique mais dans le procédé d'implémentation mis en
   place, donc cet algorithme n'a pas fait l'objet d'un travail
   particulier). Il utilise la méthode =compare= fournie avec le
   projet de départ (qui est un coefficient de Dice) :

   #+BEGIN_SRC C
   Entrée : m1 et m2 deux ensembles de chaînes
            lexicales représentant deux messages
   score ← 0
   Pour toute chaîne lexicale c1 dans m1 :
       Pour toute chaîne lexicale c2 dans m2 :
           score ← score + coefficient de Dice de c1 et c2
       Fin pour
   Fin pour
   Retourner score / |m1| / |m2|
   #+END_SRC
* Utilisation du logiciel
  Afin de rendre le développement du logiciel plus aisé sous Netbeans
  et autres IDEs différents d'Eclipse, le projet Eclipse a été
  transformé en projet Maven. Le résultat de cette transformation a
  été mis à disposition du reste du groupe sur [[https://github.com/m09/teach-uima-project/releases/tag/v1.0][github]].

  En conséquence, packager le logiciel en une jar est un simple appel
  maven :

  #+BEGIN_SRC shell
  cd path/to/project/folder
  mvn package
  #+END_SRC
  
  Pour lancer le workflow de construction du réseau de collocation,
  il faut ensuite appeler java de la manière suivante :
  
  #+BEGIN_SRC shell
  java -cp target/linkInterMessageDetector-0.0.1-SNAPSHOT-jar-with-dependencies.jar \
      linkInterMessageDetector.wf.CollocationNetworkBuilderWF
  #+END_SRC

  Le workflow de création des chaînes lexicales nécessite quand à lui
  une augmentation de la mémoire disponible (1024m sont suffisants
  chez moi mais ralentissent la fin de l'exécution−le nettoyage du tas
  prenant visiblement beaucoup de temps−2048m comme présenté
  ci-dessous sont donc plus confortables si la machine de test le
  permet). L'appel est alors :

  #+BEGIN_SRC shell
  java -Xmx2048m \
      -cp target/linkInterMessageDetector-0.0.1-SNAPSHOT-jar-with-dependencies.jar \
      linkInterMessageDetector.wf.LexicalChainsBuilderWF
  #+END_SRC

  Pour le workflow de détection de liens l'appel est :

  #+BEGIN_SRC shell
  java -Xmx2048m \
      -cp target/linkInterMessageDetector-0.0.1-SNAPSHOT-jar-with-dependencies.jar \
      linkInterMessageDetector.wf.LinksDetectionWF
  #+END_SRC

  Note : aucune récupération d'arguments n'a été implémentée pour
  configurer les workflows. Il faut donc modifier directement les
  paramètres des AEs dans les classes des workflows voulus pour lancer
  une exécution avec des paramètres particuliers. En particulier, il
  convient de bien renseigner les chemins des ressources et outputs
  afin que le logiciel fonctionne correctement. Il faut repackager le
  logiciel pour que les changements prennent effet.

  Il est aussi important d'exécuter ces 3 workflows dans l'odre
  proposé ci-dessous pour un premier run car les fichiers outputs ne
  sont pas fournis /a priori/ et chaque workflow dépend du précédent.
